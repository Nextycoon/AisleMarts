#!/usr/bin/env python3
"""
üåçüí∞ü§ñ‚ú®üöÄ ULTIMATE LIVE PRODUCTION TESTING - AisleMarts Backend Validation
Beyond Series A to Global Deployment - Comprehensive Backend Testing Suite

This script tests all production deployment systems, advanced analytics, 
AI Super Agent, and complete ecosystem validation for AisleMarts.

ULTRA PRIORITY 1 - NEW LIVE PRODUCTION SYSTEMS:
1. Production Deployment (/api/production/health) - Global deployment readiness
2. Production Status (/api/production/status) - Live production metrics
3. Global Deployment (/api/production/deploy-global) - Multi-region deployment
4. Auto Scaling (/api/production/setup-auto-scaling) - Enterprise scaling
5. Enterprise Security (/api/production/implement-security) - Bank-level security
6. CDN Configuration (/api/production/configure-cdn) - Global edge computing
7. Live Metrics (/api/production/live-metrics) - Real-time production data

ULTRA PRIORITY 2 - ADVANCED ANALYTICS & BUSINESS INTELLIGENCE:
8. Advanced Analytics Health (/api/advanced-analytics/health) - BI system status
9. Real-time Business Metrics (/api/advanced-analytics/real-time-metrics) - Live business data
10. Predictive Analysis (/api/advanced-analytics/predictive-analysis) - ML forecasting
11. AI Performance Analytics (/api/advanced-analytics/ai-performance) - AI optimization
12. Executive Dashboard (/api/advanced-analytics/executive-dashboard) - C-level metrics
13. Vendor Success Analytics (/api/advanced-analytics/vendor-success) - Vendor intelligence
14. Market Intelligence (/api/advanced-analytics/market-intelligence) - Competitive analysis
15. Financial Projections (/api/advanced-analytics/financial-projections) - Series A readiness

ULTRA PRIORITY 3 - AI SUPER AGENT SYSTEM VALIDATION:
16. AI Super Agent Health (/api/ai-super-agent/health) - Crown jewel validation
17. AI Capabilities (/api/ai-super-agent/capabilities) - 6 AI assistants status
18. AI Processing (/api/ai-super-agent/process) - Core AI functionality
19. AI Analytics (/api/ai-super-agent/analytics) - AI performance metrics
20. AI Demo Mode (/api/ai-super-agent/demo) - Investor presentation mode

Expected Success Rate: 95%+ (Production Ready)
"""

import asyncio
import aiohttp
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
import os
import sys

# Get backend URL from environment
BACKEND_URL = os.getenv('EXPO_PUBLIC_BACKEND_URL', 'https://social-tiktok-mart.preview.emergentagent.com')
API_BASE = f"{BACKEND_URL}/api"

class AisleMartsProductionTester:
    def __init__(self):
        self.session = None
        self.results = {
            "production_deployment": [],
            "advanced_analytics": [],
            "ai_super_agent": [],
            "ecosystem_validation": [],
            "performance_metrics": {},
            "overall_stats": {
                "total_tests": 0,
                "passed": 0,
                "failed": 0,
                "success_rate": 0.0,
                "total_time": 0.0
            }
        }
        self.start_time = time.time()

    async def __aenter__(self):
        """Async context manager entry"""
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()

    async def test_endpoint(self, endpoint: str, method: str = "GET", data: Dict = None, 
                          category: str = "general", description: str = "") -> Dict[str, Any]:
        """Test a single endpoint and return results"""
        test_start = time.time()
        url = f"{API_BASE}{endpoint}"
        
        try:
            if method.upper() == "GET":
                async with self.session.get(url) as response:
                    response_data = await response.json()
                    status_code = response.status
            elif method.upper() == "POST":
                async with self.session.post(url, json=data) as response:
                    response_data = await response.json()
                    status_code = response.status
            else:
                raise ValueError(f"Unsupported method: {method}")
            
            test_time = time.time() - test_start
            success = 200 <= status_code < 300
            
            result = {
                "endpoint": endpoint,
                "method": method,
                "description": description,
                "status_code": status_code,
                "success": success,
                "response_time": round(test_time, 3),
                "response_data": response_data if success else None,
                "error": None if success else f"HTTP {status_code}",
                "timestamp": datetime.utcnow().isoformat()
            }
            
            # Add to appropriate category
            self.results[category].append(result)
            
            # Update overall stats
            self.results["overall_stats"]["total_tests"] += 1
            if success:
                self.results["overall_stats"]["passed"] += 1
                print(f"‚úÖ {description}: {status_code} ({test_time:.3f}s)")
            else:
                self.results["overall_stats"]["failed"] += 1
                print(f"‚ùå {description}: {status_code} ({test_time:.3f}s)")
            
            return result
            
        except Exception as e:
            test_time = time.time() - test_start
            result = {
                "endpoint": endpoint,
                "method": method,
                "description": description,
                "status_code": 0,
                "success": False,
                "response_time": round(test_time, 3),
                "response_data": None,
                "error": str(e),
                "timestamp": datetime.utcnow().isoformat()
            }
            
            self.results[category].append(result)
            self.results["overall_stats"]["total_tests"] += 1
            self.results["overall_stats"]["failed"] += 1
            print(f"‚ùå {description}: ERROR - {str(e)} ({test_time:.3f}s)")
            
            return result

    async def test_production_deployment_systems(self):
        """Test all production deployment capabilities"""
        print("üöÄ Testing Production Deployment Systems...")
        
        # Test production health
        await self.test_endpoint(
            "/production/health",
            description="Production deployment health check",
            category="production_deployment"
        )
        
        # Test production status
        await self.test_endpoint(
            "/production/status",
            description="Live production metrics and status",
            category="production_deployment"
        )
        
        # Test global deployment
        await self.test_endpoint(
            "/production/deploy-global",
            method="POST",
            description="Global multi-region deployment",
            category="production_deployment"
        )
        
        # Test auto scaling
        await self.test_endpoint(
            "/production/setup-auto-scaling",
            method="POST",
            description="Enterprise auto-scaling setup",
            category="production_deployment"
        )
        
        # Test enterprise security
        await self.test_endpoint(
            "/production/implement-security",
            method="POST",
            description="Bank-level security implementation",
            category="production_deployment"
        )
        
        # Test CDN configuration
        await self.test_endpoint(
            "/production/configure-cdn",
            method="POST",
            description="Global CDN and edge computing",
            category="production_deployment"
        )
        
        # Test live metrics
        await self.test_endpoint(
            "/production/live-metrics",
            description="Real-time production data",
            category="production_deployment"
        )
        
        # Test deployment regions
        await self.test_endpoint(
            "/production/regions",
            description="Global deployment regions",
            category="production_deployment"
        )
        
        # Test production demo mode
        await self.test_endpoint(
            "/production/demo",
            description="Series A investor demo mode",
            category="production_deployment"
        )

    async def test_advanced_analytics_systems(self):
        """Test advanced analytics and business intelligence"""
        print("üìä Testing Advanced Analytics & Business Intelligence...")
        
        # Test analytics health
        await self.test_endpoint(
            "/advanced-analytics/health",
            description="Advanced analytics system health",
            category="advanced_analytics"
        )
        
        # Test real-time business metrics
        await self.test_endpoint(
            "/advanced-analytics/real-time-metrics",
            description="Live business metrics and KPIs",
            category="advanced_analytics"
        )
        
        # Test predictive analysis
        await self.test_endpoint(
            "/advanced-analytics/predictive-analysis",
            description="ML-powered forecasting and predictions",
            category="advanced_analytics"
        )
        
        # Test AI performance analytics
        await self.test_endpoint(
            "/advanced-analytics/ai-performance",
            description="AI system performance metrics",
            category="advanced_analytics"
        )
        
        # Test executive dashboard
        await self.test_endpoint(
            "/advanced-analytics/executive-dashboard",
            description="C-level executive dashboard",
            category="advanced_analytics"
        )
        
        # Test vendor success analytics
        await self.test_endpoint(
            "/advanced-analytics/vendor-success",
            description="Vendor intelligence and success metrics",
            category="advanced_analytics"
        )
        
        # Test market intelligence
        await self.test_endpoint(
            "/advanced-analytics/market-intelligence",
            description="Competitive analysis and market data",
            category="advanced_analytics"
        )
        
        # Test financial projections
        await self.test_endpoint(
            "/advanced-analytics/financial-projections",
            description="Series A financial modeling",
            category="advanced_analytics"
        )
        
        # Test analytics demo mode
        await self.test_endpoint(
            "/advanced-analytics/demo",
            description="Analytics investor demo mode",
            category="advanced_analytics"
        )

    async def test_ai_super_agent_system(self):
        """Test AI Super Agent - the crown jewel"""
        print("ü§ñ Testing AI Super Agent System...")
        
        # Test AI health
        await self.test_endpoint(
            "/ai-super-agent/health",
            description="AI Super Agent health and capabilities",
            category="ai_super_agent"
        )
        
        # Test AI capabilities status
        await self.test_endpoint(
            "/ai-super-agent/capabilities?user_id=test_user_123",
            description="6 AI assistants status check",
            category="ai_super_agent"
        )
        
        # Test AI processing for each capability
        capabilities = [
            "personal_shopper",
            "price_optimizer", 
            "trend_predictor",
            "style_advisor",
            "sustainability_guide",
            "deal_hunter"
        ]
        
        for capability in capabilities:
            await self.test_endpoint(
                "/ai-super-agent/process",
                method="POST",
                data={
                    "capability": capability,
                    "user_input": f"Help me with {capability.replace('_', ' ')} recommendations",
                    "user_id": "test_user_123",
                    "context": {"demo_mode": True}
                },
                description=f"AI {capability.replace('_', ' ').title()} processing",
                category="ai_super_agent"
            )
        
        # Test live insights
        await self.test_endpoint(
            "/ai-super-agent/insights?user_id=test_user_123&limit=10",
            description="Live AI insights generation",
            category="ai_super_agent"
        )
        
        # Test quick actions
        quick_actions = [
            "find_deals",
            "price_check",
            "style_advice",
            "trend_analysis",
            "eco_options",
            "personal_shop"
        ]
        
        for action in quick_actions:
            await self.test_endpoint(
                "/ai-super-agent/quick-action",
                method="POST",
                data={
                    "action": action,
                    "user_id": "test_user_123",
                    "context": {"demo_mode": True}
                },
                description=f"Quick action: {action.replace('_', ' ').title()}",
                category="ai_super_agent"
            )
        
        # Test AI analytics
        await self.test_endpoint(
            "/ai-super-agent/analytics?user_id=test_user_123&timeframe=7d",
            description="AI performance analytics dashboard",
            category="ai_super_agent"
        )
        
        # Test AI demo mode
        await self.test_endpoint(
            "/ai-super-agent/demo",
            description="AI Super Agent investor demo",
            category="ai_super_agent"
        )

    async def test_complete_ecosystem(self):
        """Test complete ecosystem validation"""
        print("üåç Testing Complete Ecosystem...")
        
        # Core system health checks
        ecosystem_endpoints = [
            ("/health", "Core API health check"),
            ("/lead-economy/health", "Lead Economy (0% commission model)"),
            ("/global-languages/health", "Global Languages (89 languages)"),
            ("/currency/health", "Currency-Infinity (185+ currencies)"),
            ("/voice-ai/health", "Voice AI Shopping (113+ languages)"),
            ("/ar-visualization/health", "AR/VR Commerce"),
            ("/creator-economy/health", "Creator Economy platform"),
            ("/sustainability/health", "Sustainability & ESG tracking"),
            ("/premium-membership/health", "Premium Membership (4-tier)"),
            ("/e2ee/health", "End-to-End Encryption"),
            ("/kms/health", "Key Management System"),
            ("/enhanced/health", "Enhanced Features"),
            ("/business/health", "Business Tools"),
            ("/ops/health", "Operational Systems"),
            ("/international/health", "International Expansion"),
            ("/rewards/health", "Rewards System"),
            ("/family-safety/health", "BlueWave Family Safety"),
            ("/business-console/health", "Business Console"),
            ("/tiktok-features/health", "TikTok-Style Social Commerce"),
            ("/universal-ai/health", "Universal Commerce AI Hub"),
            ("/city-scale/health", "City Scale Operations"),
            ("/digital-commerce/health", "Digital Commerce Platform")
        ]
        
        for endpoint, description in ecosystem_endpoints:
            await self.test_endpoint(
                endpoint,
                description=description,
                category="ecosystem_validation"
            )

    async def test_concurrent_performance(self):
        """Test concurrent performance and scalability"""
        print("‚ö° Testing Concurrent Performance...")
        
        # Test concurrent requests to critical endpoints
        concurrent_tests = [
            "/production/health",
            "/advanced-analytics/health", 
            "/ai-super-agent/health",
            "/currency/health",
            "/rewards/health"
        ]
        
        # Run 20 concurrent requests
        start_time = time.time()
        tasks = []
        
        for _ in range(20):
            for endpoint in concurrent_tests:
                task = self.test_endpoint(
                    endpoint,
                    description=f"Concurrent test: {endpoint}",
                    category="ecosystem_validation"
                )
                tasks.append(task)
        
        # Execute all concurrent requests
        await asyncio.gather(*tasks, return_exceptions=True)
        
        concurrent_time = time.time() - start_time
        self.results["performance_metrics"]["concurrent_test"] = {
            "total_requests": len(tasks),
            "total_time": round(concurrent_time, 3),
            "requests_per_second": round(len(tasks) / concurrent_time, 2),
            "average_response_time": round(concurrent_time / len(tasks), 3)
        }

    def calculate_final_stats(self):
        """Calculate final statistics"""
        total_time = time.time() - self.start_time
        total_tests = self.results["overall_stats"]["total_tests"]
        passed = self.results["overall_stats"]["passed"]
        
        self.results["overall_stats"]["total_time"] = round(total_time, 3)
        self.results["overall_stats"]["success_rate"] = round((passed / total_tests * 100), 1) if total_tests > 0 else 0.0
        self.results["overall_stats"]["average_response_time"] = round(total_time / total_tests, 3) if total_tests > 0 else 0.0

    def print_results(self):
        """Print comprehensive test results"""
        print("\n" + "="*80)
        print("üåçüí∞ü§ñ‚ú®üöÄ ULTIMATE LIVE PRODUCTION TESTING RESULTS")
        print("="*80)
        
        # Overall Statistics
        stats = self.results["overall_stats"]
        print(f"\nüìä OVERALL STATISTICS:")
        print(f"   Total Tests: {stats['total_tests']}")
        print(f"   Passed: {stats['passed']}")
        print(f"   Failed: {stats['failed']}")
        print(f"   Success Rate: {stats['success_rate']}%")
        print(f"   Total Time: {stats['total_time']}s")
        print(f"   Average Response Time: {stats.get('average_response_time', 0)}s")
        
        # Category Results
        categories = [
            ("production_deployment", "üöÄ PRODUCTION DEPLOYMENT SYSTEMS"),
            ("advanced_analytics", "üìä ADVANCED ANALYTICS & BUSINESS INTELLIGENCE"),
            ("ai_super_agent", "ü§ñ AI SUPER AGENT SYSTEM"),
            ("ecosystem_validation", "üåç COMPLETE ECOSYSTEM VALIDATION")
        ]
        
        for category_key, category_name in categories:
            results = self.results[category_key]
            if results:
                passed = sum(1 for r in results if r["success"])
                total = len(results)
                success_rate = round((passed / total * 100), 1) if total > 0 else 0.0
                
                print(f"\n{category_name}:")
                print(f"   Tests: {total} | Passed: {passed} | Success Rate: {success_rate}%")
                
                # Show failed tests
                failed_tests = [r for r in results if not r["success"]]
                if failed_tests:
                    print(f"   ‚ùå FAILED TESTS:")
                    for test in failed_tests:
                        print(f"      - {test['endpoint']}: {test['error']}")
                else:
                    print(f"   ‚úÖ ALL TESTS PASSED")
        
        # Performance Metrics
        if "concurrent_test" in self.results["performance_metrics"]:
            perf = self.results["performance_metrics"]["concurrent_test"]
            print(f"\n‚ö° CONCURRENT PERFORMANCE:")
            print(f"   Requests: {perf['total_requests']}")
            print(f"   Total Time: {perf['total_time']}s")
            print(f"   Requests/Second: {perf['requests_per_second']}")
            print(f"   Avg Response Time: {perf['average_response_time']}s")
        
        # Series A Readiness Assessment
        overall_success = stats['success_rate']
        print(f"\nüéØ SERIES A READINESS ASSESSMENT:")
        
        if overall_success >= 95:
            print("   üèÜ BEYOND SERIES A READY - GLOBAL DEPLOYMENT QUALITY")
            print("   Status: LIVE PRODUCTION READY")
        elif overall_success >= 90:
            print("   ‚úÖ SERIES A READY - PRODUCTION QUALITY")
            print("   Status: INVESTOR DEMO READY")
        elif overall_success >= 80:
            print("   ‚ö†Ô∏è  NEAR SERIES A READY - MINOR ISSUES")
            print("   Status: NEEDS MINOR FIXES")
        else:
            print("   ‚ùå NOT SERIES A READY - CRITICAL ISSUES")
            print("   Status: NEEDS MAJOR FIXES")
        
        print("\n" + "="*80)

async def main():
    """Main testing function"""
    print("üåçüí∞ü§ñ‚ú®üöÄ ULTIMATE LIVE PRODUCTION TESTING - AISLEMARTS")
    print("Beyond Series A to Global Deployment - Backend Validation")
    print("="*80)
    
    async with AisleMartsProductionTester() as tester:
        # Run all test suites
        await tester.test_production_deployment_systems()
        await tester.test_advanced_analytics_systems()
        await tester.test_ai_super_agent_system()
        await tester.test_complete_ecosystem()
        await tester.test_concurrent_performance()
        
        # Calculate and display results
        tester.calculate_final_stats()
        tester.print_results()
        
        # Save results to file
        with open('/app/production_test_results.json', 'w') as f:
            json.dump(tester.results, f, indent=2)
        
        print(f"\nüìÑ Detailed results saved to: /app/production_test_results.json")
        
        return tester.results["overall_stats"]["success_rate"]

if __name__ == "__main__":
    try:
        success_rate = asyncio.run(main())
        
        # Exit with appropriate code
        if success_rate >= 90:
            sys.exit(0)  # Success
        else:
            sys.exit(1)  # Failure
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Testing interrupted by user")
        sys.exit(2)
    except Exception as e:
        print(f"\n‚ùå Testing failed with error: {e}")
        sys.exit(3)